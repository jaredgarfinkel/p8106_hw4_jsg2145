---
title: "p8106_hw4_jsg2145"
author: "Jared Garfinkel"
date: "4/25/2020"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(rpart)
library(rpart.plot)
library(caret)
library(ISLR)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Part a

```{r}
library(lasso2)
data(Prostate)

x <- model.matrix(lpsa~.,Prostate)[,-1]
y <- Prostate$lpsa
```

```{r}
tree1 <- rpart(lpsa~., Prostate, control = rpart.control(cp = .0001))
rpart.plot(tree1)
tree1$cptable

cpTable <- printcp(tree1)
plotcp(tree1)
minErr <- which.min(cpTable[,4])
# minimum cross-validation error
tree3 <- prune(tree1, cp = cpTable[minErr,1])
# 1SE rule
tree4 <- prune(tree1, cp = cpTable[cpTable[,4]<cpTable[minErr,4]+cpTable[minErr,5],1][1])

rpart.plot(tree3)
rpart.plot(tree4)
```

```{r rpart in caret, cache = TRUE}
ctrl1 = trainControl(method = "repeatedcv", number = 10, repeats = 5)

set.seed(22)
tree_caret_cv = train(x, y, method = "rpart",
                   tuneGrid = data.frame(cp = seq(.001, 1, length = 1000)),
                   trControl = ctrl1)

tree_caret_cv$bestTune

ggplot(tree_caret_cv, highlight = TRUE)
tree_caret_cv$finalModel$cptable
rpart.plot(tree_caret_cv$finalModel)

set.seed(22)
tree_caret_1se <- train(x, y,
                   method = "rpart",
                   tuneGrid = data.frame(cp = seq(.001, 1, length = 1000)), 
                   trControl = trainControl(method = "repeatedcv", number = 10, repeats = 5,
                                            selectionFunction = "oneSE"))

tree_caret_1se$bestTune

ggplot(tree_caret_1se, highlight = TRUE) + theme_bw()
tree_caret_1se$finalModel$cptable
rpart.plot(tree_caret_1se$finalModel)

set.seed(22)
resamp <- resamples(list(minErr = tree_caret_cv,
                         oneSE = tree_caret_1se))

ggplot(resamp)
summary(resamp)
```

## Part b

```{r}
final_tree = rpart(formula = lpsa ~ ., data = Prostate, control = rpart.control(cp = 0.1))
rpart.plot(final_tree)
```

## Part c

```{r, cache = TRUE}
bagging_grid <- expand.grid(mtry = 8,
                       splitrule = "variance",
                       min.node.size = 1:20)
set.seed(22)
bagging_fit <- train(x, y, 
                method = "ranger",
                tuneGrid = bagging_grid,
                trControl = ctrl1,
                importance = "impurity")

ggplot(bagging_fit, highlight = TRUE)
bagging_fit$results[which.min(bagging_fit$results[,5]),]
barplot(sort(ranger::importance(bagging_fit$finalModel), 
             decreasing = FALSE), 
        las = 2, 
        horiz = TRUE, 
        cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred",
                                          "white",
                                          "darkblue"))(19))
```

## Part d

```{r, cache = TRUE}
randfor_grid <- expand.grid(mtry = 1:7,
                       splitrule = "variance",
                       min.node.size = 1:15)
set.seed(22)
randfor_fit <- train(x, y, 
                method = "ranger",
                tuneGrid = randfor_grid,
                trControl = ctrl1,
                importance = 'permutation')

ggplot(randfor_fit, highlight = TRUE)

randfor_fit$results[which.min(randfor_fit$results[,5]),]

barplot(sort(ranger::importance(randfor_fit$finalModel), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```

## Part e

```{r, cache = TRUE}
gbm_grid <- expand.grid(
  n.trees = seq(1, 5000, 100), 
  interaction.depth = 2:10,
  shrinkage = c(0.001,0.003,0.005), 
  n.minobsinnode = 1)

set.seed(22)
gbm_fit <- train(x, y,
                 method = "gbm",
                 tuneGrid = gbm_grid,
                 trControl = ctrl1,
                 verbose = FALSE)

ggplot(gbm_fit, highlight = T) + theme_bw()

summary(gbm_fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

## Part f

```{r}
resamp2 = resamples(list(minErr = tree_caret_cv, 
                         min_1se = tree_caret_1se,
                         randomForest = randfor_fit,
                         boosting = gbm_fit))
summary(resamp2)
bwplot(resamp2, metric = "RMSE")
```

# Problem 2

## Problem 2a

```{r}
data(OJ)
oj_data = OJ %>% 
  janitor::clean_names()
# create a training set containing 800 obs
set.seed(22)
rowTrain = createDataPartition(y = oj_data$purchase,
                               p = 799/1070,
                               list = F)
train_data = oj_data[rowTrain, ]
test_data = oj_data[-rowTrain, ]
# check whether there is 800 obs
dim(train_data)
```

```{r}
x_train = train_data[,-1]
y_train = pull(train_data, purchase)

x_test = test_data[,-1]
y_test = pull(test_data, purchase)
```

```{r, cache = TRUE}
ctrl2 <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
set.seed(22)

fit_oj_cv <- train(x_train, y_train,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-20,-5, len = 20))),
                   trControl = ctrl2,
                   metric = "ROC")

ggplot(fit_oj_cv, highlight = TRUE)
# optimal tree size is 17 with smallest CV error
fit_oj_cv$finalModel$cptable
# plot of tree
rpart.plot(fit_oj_cv$finalModel)
# predict response on test data
pred = predict(fit_oj_cv, newdata = test_data,
               type = "raw");pred
# test classification error rate
1 - mean(test_data$purchase == pred)
```

## Part b

```{r, cache = TRUE}
rf.grid_oj <- expand.grid(mtry = seq(2,12,2),
                       splitrule = "gini",
                       min.node.size = seq(20,100,5))
set.seed(1) 
rf.fit_oj <- train(purchase ~ ., train_data,
                method = "ranger",
                tuneGrid = rf.grid_oj,
                metric = "ROC",
                importance = "impurity",
                trControl = ctrl2)
# rf plot
ggplot(rf.fit_oj, highlight = TRUE)
# compare variable importance
barplot(sort(ranger::importance(rf.fit_oj$finalModel), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
# predict on test data
pred2 = predict(rf.fit_oj, newdata = test_data,
               type = "raw");pred
# test error rate
1 - mean(test_data$purchase == pred2)
```

